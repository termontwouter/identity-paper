---
title: European digital identity
subtitle: A missed opportunity?
short-title: EUDI -- A missed opportunity?
author:
  - id: termont
    name:
      given: Wouter
      family: Termont
    orcid: 0000-0002-2968-1394
    email: wouter.termont@ugent.be
    corresponding: true
    attributes:
      corresponding: true
    affiliation:
      - name: Ghent University -- imec
        department: IDLab
        group: Knowledge On Web Scale
        url: https://knows.idlab.ugent.be/
        country: BE
        id: knows
  - id: esteves
    name:
      given: Esteves
      family: Beatriz
    degree: PhD
    orcid: 0000-0003-0259-7560
    email: beatriz.esteves@ugent.be
    affiliation:
      - ref: knows
date: 2026-01-19
date-modified: last-modified
thanks: |
  This research was funded by [SolidLab Vlaanderen](https://solidlab.be/)
  (Flemish Government, EWI and RRF project VV023/10)
# published: A SolidLab white paper
# code-repo: <https://github.com/termontwouter/identity-paper>
license: CC BY
copyright:
  holder: IDLab (UGent -- imec)
  year: 2026
citation:
  type: report
  issued: 2026-01-20
  # available-date: 2026-01-20
  # doi:
  # url:
  # pdf-url:
lang: en-US
langid: american
csl: ieee
bibliographystyle: ieee.csl
bibliography: references.bib
reference-section-title: References
reference-location: section
citation-location: document
notes-after-punctuation: true
link-citations: true
# link-fields: false
citeproc: false
nocite: |
  @*
toc: true
toc-depth: 2
toc-location: right
number-sections: true
# shift-heading-level-by: -1
# abbreviations: abbreviations.yml
filters:
  - nameref
  - ./_extensions/bcdavasconcelos/citetools/citeproc.lua
  - ./_extensions/bcdavasconcelos/citetools/citefield.lua
  - ./_extensions/pandoc-lua-filters/pandoc-quotes.lua
  - abstract-section
---

# Abstract {#abstract}

Recent European efforts around digital identity -- the EUDI regulation and its OpenID architecture -- aim high, but start from a narrow and ill-defined conceptualization of authentication. Based on a broader, more grounded understanding of the term, we identify several issues in the design of OpenID4VCI and OpenID4VP: insecure practices, static, and subject-bound credential types, and a limited query language restrict their application to classic scenarios of credential exchange -- already supported by existing solutions like OpenID Connect, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation -- barring dynamic, asynchronous, or automated use cases. We also debunk OpenID's 'paradigm-shifting' trust-model, which -- when compared to existing decentralized alternatives -- does not deliver any significant increase in control, privacy, and portability of personal information. Not only the technical choices limit the capabilities of the EUDI framework; also the legislation itself cannot accommodate the promise of self-sovereign identity. In particular, we criticize the introduction of institutionalized trusted lists, and discuss their economical and political risks. Their potential to decline into an exclusory, re-centralized ecosystem endangers the vision of a user-oriented identity management in which individuals are in charge. Instead, the consequences might severely restrict people in what they can do with their personal information, and risk increased linkability and monitoring. In anticipation of revisions to the EUDI regulations, we suggest several technical alternatives that overcome some of the issues with the architecture of OpenID. In particular, OAuth's UMA extension and its A4DS profile, as well as their integration in GNAP, are worth looking into. Future research into uniform query (meta-)languages is needed to address the heterogeneity of attestations and providers.

---

:::{=html}

{{< meta thanks >}}

:::

# Executive summary {#sec-exec .unnumbered .unlisted}

While offering a plausible architecture for the limited use cases accommodated by EUDI, OpenID's design does not provide significant technical innovation when compared to existing decentralized alternatives. Based on a narrow intuition of (digital) identity, both the regulations and their architecture limit their possible applications, and miss the opportunity to construct a truly uniform model of authentication on the Web.

Aggregating terminology from multiple international legal and technical standards, we define '**partial identity**' as *a subset of the measurable characteristics of an entity*, and '**identity**' as those subsets that are *sufficient to represent the entity and distinguish the entity within a given context* -- they are *pseudonyms*, whose *anonymity set* is a singleton. However, for practically all use cases, it suffices to identify *context-dependent roles*. Interpreting '**identification**' as *the exchange of such attributes as claims about a subject*, we characterize the process of '**authentication**' as *the verification of their authenticity* -- their *origin* and *integrity* -- based on *trust* in some *authority* (i.e., the issuer--holder--verifier model). It follows from these definitions that **credentials are merely certificates**: *documents attesting to the truth of certain stated facts*.

Based on these definitions, we find a mismatch between the 'paradigm-shifting' promises of EUDI and OpenID, and the actual capabilities of their combined framework. The OpenID architecture, on the one hand, is

  - based on a history of software design that disregards best practices in internet security and interoperability;
  - limited to static credential types, and an inflexible, format-dependent query languages; and
  - not applicable to dynamic or automated use cases.

Furthermore, even within classic scenarios of credential exchange, OpenIDs new trust model fails to deliver the promised benefits; for example, when compared to already existing OIDC-based solutions (SIOPv2, OIDC4IDA, OIDC Claims Aggregation):

  - OpenID's wallet offers *offline availability*, but does not increase **portability** of personal information in the usual *Bring-Your-Own-Identity* sense, nor any extra control through *informed consent*.
  - *Selective disclosure* seems like a big step forward in user **control** over personal data; we're it not that this form of *data minimization* predominantly applies to OpenID4VCI- and OpenID4VP-specific scenarios, and a lot less to other decentralized identity models.
  - Despite the change of interactions between issuer, holder, and verifier, **privacy** issues merely shift from the identity provider to the wallet provider -- or not at all when following all OpenID's precautions.

The EUDI regulations, on the other hand, are far from achieving their promise of *self-sovereign identity*. Their use of institutionalized **trusted lists** makes participation in the identity market *dependent economical and political incentives*. This risks a de facto **re-centralization of digital identity** that not only weakens the security and neutrality of the Web, but also makes the framework vulnerable to *vendor lock-ins*, *data correlation*, and *government abuse*.

In anticipation of regulatory changes, we urge to research alternatives that aim at a broader interpretation of authentication and identity. The sound foundation provided by W3C's decentralized identifiers (DID) and verifiable credential model (VC) can be complemented by frameworks based directly on OAuth. Ongoing research into the asynchronous and dynamic capabilities of User-Managed Access (UMA), Authorization for Data Spaces (A4DS), and the Grant Negotiation and Authorization Protocol (GNAP) are prime candidates; while future research is still needed into uniform query (meta-)languages, and support for more general attestation documents.

---


# Introduction {-}

The concept of **identity** plays a big role in the fields of *Data Spaces* (DS) and *Identity and Access Management* (IAM): from the personal information of an individual human being, over the information collected about legal entities, to the processing of any of the former, either through traditional methods or involving artificial intelligence.[^fn-terms] For being so important, however, definitions of the term 'identity' are surprisingly rare in legal and technical documents related to these fields.

  [^fn-terms]: Many terms exist that define certain kinds of information about a person. Coming from different backgrounds, and using different terminology, it is often hard to determine how they relate. Most general is probably **personal information** (PI), which can be *any* information related to an individual *in any way* (e.g., one's favorite color, the number of steps it usually takes one to cross the market square). **Personal data** (PD), however, as defined in the GDPR [@eu:gdpr], is personal information that "is or might be directly or indirectly linked to \[the subject] to whom such information relates" [@openid:connect], "in particular by reference to an identifier or to one or more factors specific to \[their] identity" [@eu:gdpr, art. 4]. It includes information that might not necessarily identify an individual *directly* (i.e., on its own), but might nevertheless be *indirectly* linked to them, when put in context, or when combined with other data. Examples include photographs, location data, and many technical identifiers (e.g., cookies, device identifiers, network addresses). **Personally Identifiable Information** (PII), a similar term from U.S. legislation, has a roughly equivalent definition, but is typically seen as somewhat narrower than personal data. It includes personal information that can be used to identify the individual to which that information relates [@openid:connect;@cnss:i4009], either alone (i.e., as a single data point) or "when combined with other information that is linked or linkable to a specific individual" [@nist:sp800-63]. Examples include contact details, account names, identification numbers -- but typically not the more technical identifiers, as present in personal data. A number of specific subsets of personal data -- and of PII -- are defined in the GDPR, and together form **sensitive personal data**: information of a 'private nature', requiring additional legal protection; e.g., ethnicity, sexual orientation, political opinions, religious beliefs, health-related data, and data that can be used for biometric identification. Lastly, the regulations in the scope of this paper introduce the term **Person Identification Data** (PID): the strictly smaller set of (sensitive) personal data "enabling the identity of a natural or legal person \[...] to be established" [@eu:eidas]. It consists of the most fundamental personal details, in particular those issued and verified through official, authentic sources (e.g., one's name, nationality, date of birth).

In this paper, we look into the impact this has on the recent developments around the European Digital Identity (EUDI) framework, and its technical implementation through OpenID specifications. In [@sec-identity], we present the state of the art on relevant concepts, to establish a sound foundation to start from: we attempt to aggregate a workable definition of the term 'identity' and link it to the concepts of anonymity and pseudonymity (cf. [@sec-char]); we define 'authentication' and explain the trust model around certification authorities (cf. [@sec-trust]); and we look into the difference between certificates and credentials (cf. [@sec-attestations]).

Based on this understanding, we assess the capabilities of OpenID's specifications in [@sec-arch]: we discuss the design choices inherited from OIDC (cf. [@sec-oidc]), and point out the limitations of OpenID4VCI and OpenID4VP (cf. [@sec-vci] and [-@sec-vp]). Confronted with its lack of technological innovation, we look for another reason for OpenID's new design in [@sec-paradigm]; but we conclude that it does not offer the claimed paradigm shift in portability, control or privacy (cf. [@sec-portability], [-@sec-control], and [-@sec-privacy]).

Before offering a number of alternatives to OpenID's architecture in [@sec-alt], we look into the relevant aspects of the EUDI legislation itself in [@sec-politics]: we explain the workings of trusted lists (cf. [@sec-lists]); discuss an example of the effects of their institutionalization (cf. [@sec-weakened]); and point out the implications of the economical and political incentives they give rise to ([@sec-recentral]).



# The identity of identity {#sec-identity}

Looking at European regulations, neither [@eu:eidas]{.title-short.} (eIDAS) [@eu:eidas], nor [@eu:eudi]{.title-short.} (EUDI) [@eu:eudi], or its [@eu:arf]{.title-short.} (ARF) [@eu:arf;@eu:toolbox], define the term. Likewise for many of the technical standards[^fn-institutes] -- published by major Standards Development Organizations (SDOs) -- that play a major role in the *European Digital Identity* (EUDI) framework: W3C's [@w3c:did]{.title-short.} (DID) [@w3c:did], [@w3c:vc]{.title-short.} (VC) [@w3c:vc], [@w3c:credapi]{.title-short}, and [@w3c:fedcm]{.title-short.} (FedCM) [@w3c:fedcm]; OpenID's [@openid:vci]{.title-short.} (OpenID4VCI) [@openid:vci], and [@openid:vp]{.title-short.} (OpenID4VP) [@openid:vp]. None of them contain a definition of *identity*. [@openid:connect]{.title-short.} (OIDC) [@openid:connect] merely contains the succinct definition "\[a] set of attributes related to an entity;" and -- going back even further -- in OASIS's [@oasis:glossary]{.title-short.} (SAML) [@oasis:glossary] we only find the vague "essence of an entity \[...] described by one's characteristics" (referencing the Merriam-Webster dictionary). These seem far from workable definitions for core specifications around this topic.

  [^fn-institutes]: SDOs mentioned in this section include: the World Wide Web Consortium (W3C); the OpenID Foundation (OpenID); the Organization for the Advancement of Structured Information Standards (OASIS); the International Organization for Standardization (ISO) and International Electrotechnical Commission (IEC), in particular their Joint Technical Committee 1 (JTC1); the National Institute of Standards and Technology (NIST); and the Committee on National Security Systems (CNSS).

Even when turning to meta-standards, and general glossaries of those same SDOs, like W3C's [@w3c:glossary]{.title-short.} [@w3c:glossary], and IETF's [@ietf:rfc7642]{.number.} ([@ietf:rfc7642]{.title-short.}) [@ietf:rfc7642], [@ietf:rfc3198]{.number.} ([@ietf:rfc3198]{.title-short.}) [@ietf:rfc3198], [@ietf:rfc2828]{.number.} ([@ietf:rfc2828]{.title-short.}) [@ietf:rfc2828], and [@ietf:rfc1983]{.number.} ([@ietf:rfc1983]{.title-short.}) [@ietf:rfc1983]; we search in vain. The only documents where *identity* is defined slightly more in-depth are IETF's [@ietf:rfc4949]{.number.} ([@ietf:rfc4949]{.title-short.}) [@ietf:rfc4949], and [@ietf:rfc6973]{.number.} ([@ietf:rfc6973]{.title-short.}) [@ietf:rfc6973]; OASIS's [@oasis:identity]{.title-short.} [@oasis:identity]; and a -- back then frequently updated -- document from the Technical University of Delft [@freehaven:terms]. The state of the art at the time has been captured quite well in the reports of the *Future of Identity in the Information Society* (FIDIS) project [@fidis:journal;@fidis:book].[^fn-fidis]

  [^fn-fidis]: The FIDIS *network of excellence* (NOE) was a *European Research Area* (ERA) research consortium, funded by the European Commission's *6th Framework Programme for Research and Technological Development* (FP6), with the aim to consolidate concepts and terminology related to identity, in order to foster interoperability of technologies and services towards a *European Information Society* (EIS). Its conclusions were concretely taken up by, amongst others, its spiritual successor: the FP7 *specific targeted research project* (STREP) *Privacy and Identity Management for Community Services* (**PICOS**), focused on trust and security in information and communication technologies.

Note that the aim of this paper is not to find or attempt a universal, one-size-fits-all definition of 'identity'. Even more so than many other concepts, its meaning heavily depends on the domain of governance (legal, political, technical) and the sociocultural context. What the authors point out is rather the *lack of **any** definition* of the term 'identity', neither given nor referenced, in many technical standards pertaining to this topic (and employing the term in other definitions). In particular, standards that are said to implement legislation -- in casu, EU regulations -- while neither the technical nor the legal text give a (matching) definition of this core concept, pose the real risk of operationalizing a system that does not correspond to the original economic and sociopolitical goals.

Based on the equally outdated but detailed entries in Wheeler and Wheeler's extensive [@wheeler:security]{.title.} [@wheeler:security] and [@wheeler:privacy]{.title.} [@wheeler:privacy] -- predominantly relying on U.S. institutions -- as well as a number of government-specific white papers by OpenID [@openid:govcreds;@openid:govdi], we therefore turn to the OECD, ISO/IEC, NIST, and CNSS. We find entries on *identity*, and numerous related terms, in the following sources: [@oecd:govdi]{.number.} [@oecd:govdi], [@iso:24760]{.number.} ([@iso:24760]{.title-short.}) [@iso:24760], [@iso:24765]{.number.} ([@iso:24765]{.title-short.}) [@iso:24765], [@nist:sp800-63]{.number.} ([@nist:sp800-63]{.title-short.}) [@nist:sp800-63], [@nist:sp800-103]{.number.} ([@nist:sp800-103]{.title-short.}) [@nist:sp800-103], [@nist:fips200]{.number.} ([@nist:fips200]{.title-short.}) [@nist:fips200], [@nist:fips201]{.number.} ([@nist:fips201]{.title-short.}) [@nist:fips201], and [@cnss:i4009]{.number.} ([@cnss:i4009]{.title-short.}) [@cnss:i4009]. We discuss these in the following section.


## Characteristics of (id)entities {#sec-char}

Of the handful of definitions we found, most are stated in terms of **entities**. We paraphrase: an entity is anything that has measureable *attributes* (characteristics) by which it can be *represented* (named, described) and *distinguished* (recognized) in a relevant domain of applicability [@iso:24760;@iso:24765;@ietf:rfc4949].
While most definitions of the term *attribute* turn out to be circular or self-referential,^[Respectively, depending on *identity*, or on -- not otherwise defined -- variations of itself (e.g., feature, quality, characteristic)] we manage to aggregate the following: an **attribute** is any unique piece of information that *associates* an entity with a measurable *value* of a particular *type* [@iso:27000;@cnss:i4009;@iso:24765;@ietf:rfc4949;@oecd:govdi;@eu:eidas].
An **identity** is then *any well-defined subset* of those characteristics, that together suffice to *actually* represent that entity and uniquely distinguish it from any other entity within a concrete context (domain) [@ietf:rfc6973;@nist:sp800-63;@nist:fips201;@cnss:i4009;@oecd:govdi].^[While this is a general definition, we use the term 'identity' throughout this text to refer to '*digital* identity' in particular: identity as captured, processed, and used within electronic system.]

Since any set of attributes, of which several entities share the same, can be extended to a (minimal) distinguishing superset, we also call sets of attributes **partial identities** [@iso:24760].[^fn-freehaven] Interestingly, partial identity immediately implies (partial[^fn-anonymity]) **anonymity**: "a state of an individual in which an observer \[...] cannot identify \[it] within a set of other individuals" [@ietf:rfc6973]. This "set of individuals that have the same attributes, making them indistinguishable from each other \[for a particular observer]," is an entity's *anonymity set* [@ietf:rfc6973]. It is clear that any nontrivial entity has many partial identities, and therefore many different anonymity sets.[^fn-anonymization]

  [^fn-freehaven]: [@freehaven:terms]{.author.} express this in a witty way: "An identity is any subset of attributes of an individual which identifies this individual within any set of individuals. So usually there is no such thing as 'the identity', but several of them." [@freehaven:terms]

  [^fn-anonymity]: While identity can be qualified as partial, anonymity is typically not; though one could conceive of *full* anonymity as the state in which the anonymity set contains all entities of the domain.

  [^fn-anonymization]: To count as anonymized data under EU legislation [@eu:gdpr], the process obtaining the partial identity -- thereby removing actual identifiability -- from the proper one must be (reasonably) irreversible.

A number~of sources emphasize the distinction between identities and **identifiers** [@w3c:identity;@oasis:glossary], but they fail to point out the precise difference: both are unique sets of characteristics. While they are often atomic strings (e.g., labels, serial numbers, indexes), they can also be more complex or combined attribute structures (e.g., names, addresses, bibliographic references) [@iso:24765;@ietf:rfc4949;@cnss:i4009]. If anything, they are *minimal* (i.e., irreducible) identities, in which the removal of a single attribute reduces it to a mere *partial* identity. This aligns with the recurring view that identifiers represent (other) *identities* -- larger (super)sets of attributes -- rather than *entities* [@iso:24760;@iso:24765;@openid:connect;@nist:fips201;@cnss:i4009;@w3c:identity].

An entity potentially has multiple *proper* identities too, both in different contexts as within a single one. We use the term **principal** (*user*) to refer to the particular identity, by which the active party[^fn-party] is known in the context of a specific (user) session [@ietf:rfc2828;@cnss:i4009;@ietf:rfc4949;@ietf:rfc6973;@iso:24760].^[A continuous period of transactions between an entity and an information system [@ietf:rfc4949].] When a principal identity can be "associated with multiple interactions with \[the same entity]" [@nist:sp800-63], viz. because they contain at least "the minimal identity information sufficient to \[...] establish it as a link" [@iso:24760], it is also called a **persona** [@cnss:i4009;@w3c:vc], or **pseudonym**.[^fn-persona] The latter term is typically used when it concerns a privacy-preserving *identifier* [@oasis:glossary], which does not let the verifier infer anything else regarding that entity [@nist:sp800-63], such as the entity's identifier at another relying parties [@openid:connect].[^fn-pseudonymization]

  [^fn-party]: A party consists of "one or more \[entities] participating in some process or communication" [@oasis:glossary], which are "considered to have \[at least] some of the rights, powers, and duties of a natural person" [@iso:24760]. They are a natural or legal person that has agency, and can therefore be held "responsible for their actions and the actions of their agents" [@iso:24765].

  [^fn-persona]: Some sources hold pseudonyms and personas to conceal or protecting an entity's *true identity* [@cnss:i4009;@ietf:rfc4949] However, it is unclear what such a '*true* identity' would be.

  [^fn-pseudonymization]: In EU law [@eu:gdpr], data is called pseudonymized if on its own it can not be attributed to a specific subject entity, but -- unlike anonymized data -- it can be (reasonably) reversed into a proper identity when in the possession of additional information.


## On trust in authorities {#sec-trust}

The act of *presenting* identity information to a system -- making **claims** about the attributes of an entity (the **subject**)[^fn-identification] -- and the system subsequently *validating*[^fn-validation] the collected information in order to *recognize* the represented entity -- as (uniquely) distinct within a context -- is called **identification** [@iso:24760;@nist:sp800-63;@cnss:i4009;@ietf:rfc2828;@ietf:rfc4949]. **Authentication**, on the other hand, is the more formal process of establishing a sufficient level of *assurance* in the **authenticity**[^fn-authenticity] [@oasis:glossary;@openid:connect;@iso:24760;@nist:fips201;@oecd:govdi] of an entity. It involves **verifying**[^fn-verification] the *origin* (i.e., source) and *integrity*[^fn-integrity] of information [@cnss:i4009;@nist:sp800-57;@nist:sp800-175;@ietf:rfc4949;@eu:eudi], in order to determine the *binding* between the presented claims [@openid:connect;@iso:24760;@nist:sp800-57;@nist:sp800-175;@ietf:rfc4949;@ietf:rfc1983;@nist:fips200;@iso:29115;@nist:sp800-63;@ietf:rfc2504;@oasis:glossary].[^fn-authverif] In the context of (user) identification, authentication determines "the degree to which the identity \[...] can be proved to be the one claimed" [@iso:24765], typically by ensuring that the entity controls (e.g., is, possesses, or knows) a valid token (authenticator), bound to their account -- e.g., "the private key of a previously-registered public key" [@w3c:webauthn] -- to demonstrate that they are associated with that account [@nist:sp800-63].[^fn-verifid]

  [^fn-identification]: In the context of identification, the subject is typically the principal itself, and the claims contained in the certificate are *identity information* [@openid:vci;@iso:29100;@ietf:rfc4949]. Because of this, the terms *subject* and *principal* are often used interchangeably [@nist:sp800-63;@cnss:i4009;@ietf:rfc3198;@iso:24760].

  [^fn-validation]: To **approve** of something (e.g., data structures, relationships), typically after *verifying* whether it satisfies certain *requirements* (e.g., soundness, correctness, completeness, consistency), often in *compliance* with some specification (e.g., standard, convention) [@ietf:rfc4949;@iso:24765]. In case of *identification* (certificates), it involves checking whether all required attributes, needed to recognize an entity in a domain, are *present*, have the *correct syntax*, and are *not expired* [@iso:24760].

  [^fn-verification]: Presenting **evidence** and checking it against previously corroborated information associated with it [@iso:24765;@iso:29115;@nist:fips201]. It determines the *truth* or *accuracy* of an (atomic) statement (e.g., fact, claim, value, signature, integrity) [@ietf:rfc4949;@openid:connect], by **proving** the "binding between the attribute and \[the entity] for which it is claimed" [@ietf:rfc4949;@iso:29115].

  [^fn-authenticity]: The property of "being genuine and able to be verified and be trusted" [@ietf:rfc2828;@ietf:rfc4949;@cnss:i4009], which, in the case of data, ensures confidence in its validity, its source (origin), and its transmission (integrity) [@nist:sp800-63].

  [^fn-integrity]: The "constancy of and confidence in data values," i.e., the assurance that the data has not been altered (e.g., changed, destroyed, lost) in an unauthorized (or accidental) manner since it was created.[@ietf:rfc4949;@nist:sp800-57;@nist:sp800-152]

  [^fn-authentication]: A few sources claim that the term is ambiguous, being either about *identity* (entity authentication), or about *integrity* (data authentication), or *origin* (source authentication) [@ietf:rfc4949;@nist:sp800-175;@nist:sp800-57]. However, such a distinction misrepresents any of these 'specific' processes as not including the other. To the contrary, it is impossible to check the origin of data without determining its integrity, and vice versa; and likewise, it is not possible to verify an identity without assuring its integrity and origin.

  [^fn-authverif]: While authentication is therefore always a form of *(id)entity verfication*, it does not only apply to user identity: "\[it] may involve any type of attribute \[...] \[claimed] on behalf of a subject or object by some other \[entity]" [@ietf:rfc4949].[^fn-authentication]

  [^fn-verifid]: Note that successful authentication does not imply that the verifier knows the concrete identity of an individual user. It only guarantees the relying party that, when multiple sessions involve the same authenticated attributes, these sessions can pertain to the same principal. The natural person behind that principal might, however, not always be the same [@w3c:webauthn].

Assurance about the authenticity of information can be based on trust in third-party **(certification) authorities** (CAs) -- the archetypal *trust service provider* [@eu:eidas] -- that vouch for its integrity and accuracy. These trusted authorities are often the primary or *authoritative sources*, who manage the life cycle of the information, keeping it accurate and up-to-date [@iso:29115];[^fn-lifecycle] as well as the *issuers*, who generate (signed) documents (**certificates**) asserting the information, and provide them to principals [@ietf:rfc4949;@w3c:vc;@openid:connect;@openid:vci;@openid:vp;@iso:24760;@iso:24765;@nist:fips201;@nist:sp800-63;@eu:eidas]. When the asserted information is about the principal themself, we also call such authorities a **credential service providers** (CSPs) or *identity providers* (IDPs), and the certificates they issue **(verifiable) credentials** -- or, to use the EUDI term, *(electronic) attestations of attributes* (EAA) [@eu:arf;@eu:eudi;@iso:24760;@ietf:rfc6973;@ietf:rfc2828;@oasis:glossary;@oecd:govdi].[^fn-services]

  [^fn-lifecycle]: A life cycle consists of the different stages involved in the management of information: its collection, initial authentication (*proofing*), and registration (storage) -- in case of a *digital identity lifecycle* also together called *enrollment* [@oecd:govdi] -- as well as its storage, issuance, revocation, expiration, preservation, and termination [@iso:24760;@ietf:rfc4949;@nist:sp800-63;@cnss:i4009].

  [^fn-services]: In [@sec-politics], we discuss different kinds of EAAs, and go deeper into the nature of trust services and their providers.

Having received a certificate, its **holders** -- the principal to whom it was issued and who controls it [@nist:sp800-103] -- can transmit it to third parties as **(verifiable) presentations** [@w3c:vc;@openid:vci;@openid:vp;@nist:sp800-63]: a selection of assertions (derived) from one or more certificates [@w3c:vc;@openid:vp], typically bound to (a key of) the holder -- to prove their legitimate possession of the certificate in question [@openid:vp]. The third party service provider, who receives a (verifiable) presentation from a principal, and performs verification to assess its authenticity, is called the **verifier** [@openid:vci;@openid:vp;@iso:24760;@w3c:vc], or **relying party** -- because it trusts and *relies* on a system of digital identity solutions to confirm the validity of the assertions [@iso:24760;@oasis:glossary;@ietf:rfc4949;@w3c:vc;@oecd:govdi].


## An anatomy of attestations {#sec-attestations}

Certificates typically consist of two distinct types of information. First, they include the actual information, as one or more **assertions**: claims made by the issuer about the *subject*, associating it with certain attribute values [@oasis:glossary;@oasis:saml;@w3c:vc;@openid:vci;@openid:connect]. Second, it contains **attestation** information: evidential (meta-)statements that describe the **authentication context**, which consists of all the "information that the relying party can require before it makes \[a] decision with respect to an authentication" [@openid:connect]. This includes the verification and revocation methods, level of assurance, security characteristics, and all the (cryptographic) **verification factors** needed to "verify the security of cryptographically protected information" [@w3c:vc]. These factors bind the asserted claims together, to the issuer and -- in case of credentials -- to the principal [@ietf:rfc4949;@iso:24760;@iso:24765;@iso:29115@cnss:i4009;@nist:fips201;@nist:sp800-63].

It is often assumed that "the holder of a credential \[...] presenting the claims \[to] the verifier is (controlled by) the subject of the claims" [@openid:security]. Some sources note certain exceptions, in which "claims in a credential can be about different subjects," not limited to assertions about the principal presenting them [@w3c:vc]; or in which the holder is not the subject of the claims (e.g., a parent presenting their child's birth certificate) [@openid:security]. These cases are in essence an abstraction from credentials towards certificates in general, though, and often lack technical support. After all, relying parties typically require information to be about the user they actually interact with during a session. Without this guarantee, malicious parties can abuse credentials (obtained through a data leak or other security breach) to impersonate someone else [@openid:security]. These scenarios underlie the idea of ***'legitimate'* holders** of a credential, often -- but not always -- the subject.

Credentials are therefore often tied to their holder, through a mechanism called **holder binding**: the addition of holder-related verification factors. This additional attestation information serves as evidence, to be corroborated with information known to be in control of the legitimate holder. It can be something the holder *has* or *knows*, or something they *are* or (typically) *do* [@iso:29115], including traditional accounts (e.g., password-based), cryptographic material (e.g., keys), biometric information (e.g., fingerprints), or any other claim linking the credential in question to information (i.e., another credential) of which the holder's identity has already been established [@openid:vci;@openid:security]. Nevertheless, even OpenID -- whose specifications indeed preclude scenarios in which holder and subject differ (cf. [@sec-vci]) -- admits that it is "important to distinguish between the information that the credential holds (about the subject) and the information that the credential is bound to (about the holder)" [@openid:security]. This makes it hard to distinguish between credentials and certificates -- not unlike between identities and identifiers.

The distinction becomes even more blurry when considering that identification does not necessarily need to be unique. In a large amount of use cases, it suffices that the relying party knows the principal's *partial* identity.[^fn-alcohol] This also becomes apparent in the strong increase of **role-based access control** mechanisms, over (purely) *identity-based* authorization; amongst others in many cloud service providers (e.g., Google Cloud, Amazon Web Services).[^fn-roles] Entities are then issued a *role certificate* -- in place of individual identifiers [@ietf:rfc7642] -- which only asserts that the entity is "\[a] member of the set of \[entities] that have identities that are assigned to the same role" [@ietf:rfc4949]. One could even go so far as to say that *roles **are** (partial) identities*, which can be assumed by different entities [@aws:iam]. Which roles -- which identities -- are distinguished depends entirely on "the context of a function delivered by a particular application" [@auth0:glossary].

  [^fn-alcohol]: One classical example is the age requirement for buying alcohol. A minimal credential, that still provides sufficent assurance to an online shop, could consist of an age -- or merely a 'yes' or 'no' -- combined with a verification factor, and signed by a government institution. The only necessary information is the binding of the holder (i.e., the buyer) with the partial identity "someone of legal age" -- an identity that could be taken up by the majority of people. Solely based on these credentials, it is impossible for the shop to know whether multiple successful checkouts were made by the same buyer or not. Without extra information (e.g., customer accounts), from the perspective of the shop, the world consists of only two identities: one over the legal age, and one not.

  [^fn-roles]: In the role-based paradigm, entities are identified by their **roles**: formal placeholders for the *functional positions* in which they can participate in interactions [@ietf:rfc4949;@iso:24765]. Roles are characterized by constraints on the behavior expected in a specific situation, i.e., a set of *connected actions* expressing *situation-dependent* attributes [@freehaven:terms]. They are typically expressed as a pre-established collection of applicable policy rules [@ietf:rfc4949;@ietf:rfc3198;@iso:24765]: "when you grant a role to a principal, you give that principal all of the permissions in that role" [@gc:iam].

When we take all these use cases into account, *the subject becomes of less and less importance*. As a concrete example, take the sale of a property. To be able to sell it, the owners should at least provide a credential (i.e., the deed) attesting that they -- as a subject -- are indeed it's legitimate proprietors. A number of other relevant documents, however, should -- while definitely bound to the property -- not necessarily mention them; e.g., an energy rating, or attestation of soil composition). Both the deed and the other documents will have to be transferred to the notary, their information verified at the relevant institutions, and subsequently processed into a bill of sale, signed by all parties involved, and passed onto the buyer. While the presence or absence of the holder--subject thus forms a single (theoretical) difference between credentials and certificates, the (practical) similarities are much more numerous. We therefore conclude that -- at least from a practical perspective -- there is *no useful distinction between credentials and (other) certificates*. Both are a "document attesting to the truth of certain stated facts" [@nist:fips201].


# OpenID's architecture for EUDI {#sec-arch}

Now, why is all of the above more than nitpicky semantic filibustering? What is the impact of this realization that identity and credentials can be *any data*? How does it affect IAM approaches in Data Spaces, and in particular the EUDI framework and its OpenID architecture? We start with a brief technical refresher.

The basis of the entire OpenID ecosystem is the OAuth 2.0 Authorization Framework [@ietf:rfc6749], which moves the responsibility for access control from the *resource server* (RS) to a separate *authorization server* (AS). The latter provides *clients* (applications) of the former with uniform **access tokens** that enable access to protected resources, in exchange for a variety of **grants**: credentials representing the resource owner's approval of the requested access (e.g., client-specific credentials, interactively obtained codes). This exchange happens at the (singular) **token endpoint**, where the client *authenticates* itself, presents its grant, and requests a certain resource scope [@ietf:rfc6749].

The OAuth 2.0 authorization server is therefore both an *authority* and a *relying party*: it issues tokens, but verifies grants. In terms of identity, it exchanges a client's *proper* identity (linked to the grant), for a *partial* identity: the token, asserting that the client has a certain authorization. Note that the input (grants) can thus be a lot more complex than the output (tokens), which also becomes apparant in the variety of grant extensions to the protocol.^[While the extensions related to tokens are at least as numerous, they mostly concern security enhancements, or additional JSON Web Token claims.]


## OpenID Connect {#sec-oidc}

All OpenID's architectures (OIDC, OpenID4VCI, OpenID4VP) are layered on top of this OAuth 2.0 design, reusing its *authorization* primitives (flows, endpoints, tokens etc.) for *authentication* purposes. OIDC hijacks OAuth 2.0's interactive authorization code flow,[^fn-oidc-flow] in which the principal authenticates themselves, and returns an **identity token** (ID token), containing a *subject identifier* -- when the client requests it -- as well as an access token, that enables the client to retrieve (additional) identity information from the authorization server's **userinfo endpoint** [@openid:connect].

  [^fn-oidc-flow]: `\label{fn-oidc-flow}`{=latex}`<a name="fn-oidc-flow"></a>`{=html}We do not go into the other flows specified in OIDC, since they are based on the insecure implicit flow from OAuth 2.0. This flow has since a long time been found vulnerable to several attack vectors, and was therefore deprecated by [@ietf:rfc9700]{.number.} ([@ietf:rfc9700]{.title-short.}) [@ietf:rfc9700] and removed in the draft of [@ietf:oauth21]{.title-short.} [@ietf:oauth21].

Even when ignoring its subtle annulment of OAuth 2.0's separation of concerns[^fn-oidc] -- an OIDC server manages *both* the identity data *and* the permissions to access those -- OIDC's architecture poses several problems. By issuing ID tokens from the same token endpoint as access tokens, OIDC has tried to shoehorn much more complex information -- *any form of identity data* -- into an API that was designed for a much simpler (yes-or-no) output. While the extra userinfo endpoint seems to accommodate for this added complexity, its interface only allows for *a flat key-value mapping* (from attribute names to JSON values), requested using a custom query language that is best described as a custom version of JSONPath [@ietf:rfc9535], or a *primitive flavor of JSON Schema* [@openid:connect]. Even in their standardized form, these JSON-based languages are structurally coupled to the JSON value tree, and thus a particular VC format, rather than to the semantics of the attestation. This limits the reusability of a query, since it "imposes an implicit reliance on \[...] the issuer's local context, such as language and culture" [@idlab:queryvc]. It also does not support queries over graphs of multiple credentials -- a requirement in even basic use cases. It is no surprise then, that -- despite the extensibility of OIDC's set of claims -- implementations typically only rely on the subject identifier, or at most a limited selection of standard claims (name, email, address, phone_number, birth date, gender).

  [^fn-oidc]: Apart from the technical consequences described in the main text, OIDC's (re)mixing of concerns also creates a lot of confusion around the semantics of core OAuth 2.0 concepts (e.g., tokens, audience, revocation). Two pertinent implications of this are implementations employing the OIDC access token to access resources (other than the userinfo endpoint); and minting identity tokens for another audiences than the client. As a consequence, implementations not always comply with the necessary security measures for OIDC's threat model.


## OpenID for Verifiable Credential Issuance {#sec-vci}

In OpenID4VCI, identity information is no longer provided as an ID token, but the essence remains the same: clients can request information as a credential using an OAuth 2.0 flow,[^fn-vci-flow] and access it at the credential endpoint -- replacing the userinfo one [@openid:vci]. Nevertheless, EUDI implementers claim that it has "several essential benefits" for the ecosystem, stating that it enhances trust, security, privacy, control, compliance, and interoperability [@igrant:eudi]. These properties are said to increase because OpenID4VCI combines the well-known flows of OIDC with the 'new' digital proofs technology of verifiable credentials -- which they claim to be harder to fake or alter. Furthermore, OpenID4VCI would achieve these benefits while preserving a straightforward user experience [@igrant:eudi].

  [^fn-vci-flow]: `\label{fn-vci-flow}`{=latex}`<a name="fn-vci-flow"></a>`{=html}Note that even in this recent specification, OpenID does not completely bar the insecure flows from OAuth 2.0 / OIDC (cf. `Footnote \ref{fn-oidc-flow}`{=latex}`the <a href="#fn-oidc-flow">note on OIDC</a>`{=html}). This is especially surprising since they refer to the deprecation of these flows by [@ietf:rfc9700]{.number.} [@ietf:rfc9700] in the OpenID4VCI *Security Considerations* [@openid:vci, ยง 13.2], the OpenID [@openid:haip]{.title-short.} [@openid:haip], and the OpenID [@openid:fapi]{.title-short.} [@openid:fapi].

Practical attempts tell a different story, though. VCs indeed provide a semantically richer alignment of credentials, but they are not inherently more expressive, nor more secure, than classic OIDC tokens. To the contrary, the OpenID4VCI specification itself lists JSON Web Tokens (JWT) with JSON Web Signatures (JWS) [@ietf:rfc7519;@ietf:rfc7515] -- similar to an OIDC token -- as a possible serialization of VCs [@openid:vci].

Moreover, while the query mechanics got a slight upgrade, it remains focused on a narrow concept of credentials. Issuers offer a number of preset **credential configurations**, specified in their metadata: particular combinations of a **credential type** with a **credential format** (e.g., "a driver's license in ISO's mdoc format", "a university degree in SD-JWT format"). Concrete credentials are then the result of applying a credential configuration to a dataset of identity information [@openid:vci]. Within these configurations, issuers can provide clients a decent amount of flexibility regarding the *packaging* (e.g., format parameters, cryptographic algorithms); yet only a single parameter addresses the actual *content* of the credential. The entire semantics, differentiating one credential (configuration) from another, must be expressed in a single string -- the *credential type*. This is only practically feasible when the offered configurations are static, and limited in number.

Using **claim descriptions** (sets of *claims path pointers*), clients can select a limited number of claims out of a larger credential (configuration) [@openid:vci], but this only adds the ability to request *subsets* of the predefined credential types offered by the issuer. As such, it is no improvement over OIDC's custom flavor of JSONPath or JSON Schema (cf. supra) -- yet is not interoperable with any of them, nor compatible with any existing OpenID client. Moreover, this approach makes the semantics of a credential dependent on the structure of the format -- and assumes that the client already knows this structure. OpenID4VCI is thus only suited for issuers offering a select assortment of distinct bundles of information, with a *fixed semantics* agreed upon out-of-band.

The design of OpenID4VCI also precludes the issuance of credentials with subjects other than the principal. In scenarios where the client is not yet known -- i.e., all cases except active offers by the issuer or updates with refresh tokens -- *the principal must (interactively) identify themself*. Since every other step of the protocol is based on *subject-agnostic* credential (configuration) identifiers, there is no way for the issuer to know about which subject a credential is requested, i.e., to which dataset to apply the credential configuration. Not only does this severely *limit the kind of credentials* that can be issued through OpenID4VCI; the need for interaction also *complicates the automation* of credential issuance. Taken together, the limitations described in this section lead us to conclude that OpenID4VCI is mainly targeted at the issuance of a handful of specific credentials, actively offered to (the client of) a known principal.[^fn-separation]

  [^fn-separation]: This conclusion is reinforced by the protocol's lack of separation between a resource server and an authorization server. While the specification explicitly states that "the Credential Issuer acts as \[a] Resource Server \[which] *might* also act as an Authorization Server."[@openid:vci] -- thus allowing for (multiple) authorization servers separate from the credential issuer -- the practicalities of the protocol prevent it from being actually feasible. The authorization servers supported by the issuer are merely declared as a flat array in its metadata. Without an offer, there is no way for clients to know which one to request a token from; nor is there a mechanism specified for dynamic discovery at the credential endpoint (e.g., as part of the error response when unauthenticated). When not (only) relying on credential offers, the issuer itself has therefore to be the (only) authorization server -- thereby severely limiting the flexibility of the framework.


## OpenID for Verifiable Credential Presentations {#sec-vp}

Credentials issued via OpenID4VCI are not meant to be requested by -- or transmitted to -- verifiers themselves. Instead, the EUDI framework defines an extra, intermediary digital identity solution: a digital **wallet**, enabling principals to request credentials from issuers, and present them to verifiers in a uniform flow [@eu:eudi]. OpenID specifies the design of these wallets in OpenID4VP [@openid:vp]. Unsurprisingly, this 'vanguard of privacy-enhanced digital wallets' suffers from many of the same issues as OIDC and OpenID4VCI, and hardly offers innovations that warrant to be called "quintessential for reinforcing digital identity management" [@igrant:eudi].

Even more than in those other specifications, the architecture of OpenID4VP forces the wallet to be both authorization server and resource server, barring more flexible scenarios in which these roles are federated or decentralized. Moreover, while said to provide an enhanced secure verification mechanism, that amplifies the credibility of digital authentication procedures [@igrant:eudi], the specification seems to double down on a variant of the insecure OAuth 2.0 implicit authorization flow, which has been deprecated [@ietf:rfc9700] and removed from OAuth 2.1 [@ietf:oauth21] (cf. `Footnotes \ref{fn-oidc-flow} and \ref{fn-vci-flow}`{=latex}`the notes <a href="#fn-oidc-flow">on OIDC</a> and <a href="#fn-vci-flow">on OpenID4VCI</a>`{=html}).

Similar to the OIDC flow, a successful OpenID4VP request results in a response containing one or more credentials -- now called VP tokens, instead of ID tokens. To request and construct these (verifiable) presentations from the credentials available to the wallet, OpenID4VP includes a custom Digital Credentials Query Language (DCQL). There is not much more to this language than some metadata around the JSONPath style *claims descriptions* of OpenID4VCI: sets of *claims path pointers* indicating the desired claims by their structural location in a known (predefined out-of-band) credential type. Rather than achieving the claimed 'comprehensive interoperability' [@igrant:eudi], the custom interfaces of the specification thus preclude a straightforward integration between conventional and contemporary OpenID technologies -- let alone other identity management frameworks.


# A paradigm shift that never was {#sec-paradigm}

Given the strong parallel between OpenID4VP and OpenID4VCI, highlighted in the previous section, one must wonder about the reasons for having two separate specifications that regulate almost identical flows of information. After all, (verifiable) credentials *are* (verifiable) presentations themselves. In OIDC, for example, relying parties get the identity token (i.e., presentation) of a principal directly from the latter's issuer.

The similarity becomes even more apparent when taking into account *self-issued credentials*. Extending OIDC, the [@openid:siop]{.title-short.} (SIOPv2) [@openid:siop] specification defines an OpenID provider (i.e., issuer) controlled by the principal -- either in the cloud or on their device, similar to a wallet -- such that "the \[principal] becomes the issuer of identity information \[i.e., tokens], signed with keys under \[their] control \[in order to] present self-attested claims directly to the \[relying party]" [@openid:siop].[^fn-self-asserted] Though the relying party's trust relationship in SIOP is directly with the principal, rather than with a third-party issuer, from a technical perspective OIDC and SIOPv2 are almost identical: only the equality of the `sub` and `iss` claims indicates the difference.[^fn-siop]

  [^fn-self-asserted]: Note that the SIOPv2 specification explicitly aligns itself with self-signed certificates, such as W3C presentations with self-asserted claims, issued by the holder themself [@openid:siop;@w3c:vc]. However, while in the W3C model "these self-asserted claims \[...] are not limited \[to] statements about the holder" [@w3c:vc], SIOPv2 tokens are limited to assertions about the principal themself -- as with the other OpenID specifications.

  [^fn-siop]: Interestingly, the SIOPv2 specification states that, unlike OIDC assertions, the information included in a SIOPv2 token is -- due to its self-asserted, self-attested nature -- **non-verifiable**: allegedly, signatures made by the principal themself can not be used to validate the origin of the signed information, "because a Self-Issued OP within the \[principal's] control does not have the legal, reputational trust of a traditional OP" [@openid:siop]. We beg to differ. The origin of the information, in case of SIOPv2, is the principal; and, by checking their signature, this origin is verified. Moreover, the lack of qualifiers like *legal*, or *reputational* -- both of which could certainly be present between a principal and a relying party -- does not change the fundamental strength of a trust relationship.

To come back to the parallel between OpenID4VCI and OpenID4VP, note the following difference with OIDC and SIOPv2. The latter pair employs a single, uniform protocol (i.e., OIDC), on top of which self-issuance is made possible. On the other hand, the former specifications are -- while functionally the same -- mechanically distinct. The question therefore remains: what are the advantages of such a double tiered framework? The specifications themselves provide no insight.

In a white paper, titled [@openid:trustmodel]{.title.} [@openid:trustmodel], OpenID calls its own architecture constitutive of a *paradigm shift*, driven by an evolution in *user-centricity*: the principal is put "in the center of the exchange between the verifier and the credential issuer" [@openid:trustmodel],[^fn-exchange] granting them more **control**, **privacy**, and **portability** over their identity information. This 'big shift' must be understood against the background of traditional **federated models**.[^fn-federation] In such systems, an issuer -- in a trust relationship with the verifier -- provides credentials 'just-in-time', i.e., each time a principal requires one for interacting with a relying party. OpenID's new architecture figures in the trend of **decentralization**,[^fn-decentralization] which they *contrast* with federation. This is surprising, since federation is actually *one of two forms of decentralization* -- the other one being distributed (peer-to-peer) models [@narayanan:decentralized].[^fn-distributed] Since the OpenID ecosystem is not a fully distributed one, we look into the benefits they attribute to their -- "beyond federated" -- decentralized approach, and how these benefits contribute to the claimed increase in *control*, *privacy*, and *portability*.

  [^fn-exchange]: Note that -- while other readings are possible -- the lack of a comma prompts a traditionally schooled reader to interpret this sentence as pertaining to an exchange between **two** parties (i.e., verifier and issuer). The principle might be at the center of it, but is -- under this interpretation -- still not a main 'partner'.

  [^fn-decentralization]: A systems model in which (some) activities or processes are not controled by a single, central authority [@johnson:decentralized]. Rather, independent decisions are made locally and autonomously by multiple parties (called *peers*), directed towards their own individual -- and possibly conflicting -- goals [@khare:decentralized].

  [^fn-distributed]: Taking the distributed models as the opposite of fully centralized models, one could think of federation as covering the entire spectrum between -- but excluding -- those extremes. As such, a federated system is one that has *multiple centers* instead of one: a "distributed network with each node \[...] being a centralized network," as it were [@peeters:decentralized].

  [^fn-federation]: A systems model in which parties collectively agree upon a division of the entire system into a set of independent 'local' domains -- *within* each of which decisions are made autonomously according to their own interests -- while interactions between them is governed by a 'global' authority [@serrano:federated;@narayanan:decentralized].


## Portability {#sec-portability}

As OpenID correctly states in their white paper, *portability* of identity information increases in a decentralized system, because principals can use *their **own** identifier(s)* at issuers and verifiers, instead of one namespaced to a specific third-party issuer and assigned to them [@openid:trustmodel]. However, this principle -- called **Bring Your Own Identity** (BYOI) -- is already a cornerstone of the *federated* model (largely popularized by OIDC). The question therefore remains: how does OpenID move beyond that in their new architecture, and what does the principal gain by it.

According to the white paper, the portability consists of the principal's ability to "control their relationship with the verifiers *independent* from third party \[issuers'] decisions or lifespan," and therefore to "present credentials to the relying parties who do *not have a federated relationship* with the credential issuer" [@openid:trustmodel]. This is a strong claim, for which the white paper provides no support, and which is neither present nor required in any of the use cases listed in the document. In fact, in the same text we find that "verifiable credentials \[...] facilitates the establishment of cross-domain trust among organizations": "verifiers need to trust the respective credential issuer," which will require "regulatory or contractual relationships on top of technical interoperability" [@openid:trustmodel] -- exactly like in the federated model.

If anything, the OpenID4VP specification increases portability in a more literal sense: storing credentials in a wallet ensures their **offline availability** in case of technical problems at the issuer's side, or -- with an on-device wallet -- internet failure.^[Then again, such functionality could also be implemented through asynchronous OIDC Claims Aggregation [@openid:aggregation].] Any claims concerning greater independence from issuers -- such as BYOI-like portability -- cannot be attributed to OpenID's new design, since it follows directly from a 'traditional' standardized federation model.


## Control {#sec-control}

According to OpenID's white paper, from a *control* perspective 'decentralization' means "not depending on one single body controlling \[...] the ecosystem," and thus enabling principals and other parties to make critical decisions, e.g., "from *which \[issuer]* to obtain what credential," and "*when* to disclose *which credential* to *which verifier*" [@openid:trustmodel]. Apart from the promised increase in portability (cf. supra), the specifications seem to rely predominantly on **informed consent** and **selective disclosure** to check this box.

*Informed consent* is indeed a cornerstone of privacy-aware technology. Verifiers and wallets should make sure that the context of a request -- including a sufficiently specific purpose -- is clear to the principal, and should obtain the latter's consent -- through explicit interaction -- before disclosing information [@openid:vp]. Again, however, this is nothing new -- OIDC already stresses its importance -- nor does it answer the question concerning the split architecture, since both OpenID4VP *and* OpenID4VCI require consent [@openid:vp;@openid:vci].

*Selective disclosure* is an interesting feature. This data minimization technique -- supported by multiple credential formats -- enables principals to select specific claims from a credential in their wallet, creating a presentation that only discloses the selected information, without revealing the rest of the credential to the verifier [@openid:vp]. This technique drastically improves the control of principals in scenarios in which credentials typically contain more claims than strictly required. It would be a strong advantage of OpenID's new design, were it not that their architecture is also the main cause of such scenarios in the first place. Splitting the wallet from the issuer, and having the latter issue reusable credentials, from which multiple presentations can be created, indeed makes it necessary for the wallet to filter which claims are disclosed to which verifiers. Other models, like OIDC, don't have this issue to begin with: the issuer creates a new credential (presentation) -- tailored to the verifier -- for each request.

Neither informed consent, nor selective disclosure therefore add a real advantage to OpenID's approach to decentralization; especially not when compared to (other) 'more traditional' federated models. With respect to control, this lack of technological progress is not core issue, though, since the regulations themselves do not allow for more (cf. [@sec-recentral]).


## Privacy {#sec-privacy}

The increase in *privacy* in decentralized models is due to the principal's ability to "*directly* present identity information to the relying parties," who "can receive and validate presented credentials without \[either the principal or the verifier] directly interacting with the issuer" [@openid:trustmodel]. The authors of the white paper claim that this 'most notable feature' -- which they say mimics physical credentials[^fn-physical] -- preserves the principal's privacy, "since \[issuers] no longer know what activity \[principals] are performing at which relying party" [@openid:trustmodel]. In particular, they state that "scenarios where the \[issuer] has no legitimate reason to know which \[relying party] the user wants to access resources from and when they do so," are not achievable with (other) federated flows [@openid:trustmodel].[^fn-issuer]

  [^fn-physical]: OpenID's analogy -- that their trust model mimics physical credentials -- does not hold well under further scrutiny. While, at first sight, a physical credential (e.g., a driver's license) might seem to contain everything a verifier (e.g., a police officer) needs to know, this is not the case. It does not merely lists assertions (e.g., name, allowed type of vehicles), but -- similar to a digital credential -- it also contains different kinds of **attestation information**. It provides information about the *authentication context* (e.g., registration number, dates of issuance and expiry), as well as verification material for *holder binding* (e.g., a picture, indication of sex and age) and *issuer binding* (e.g., a signature, holographic mark, and other anti-fraud features). In practice, it is true that verifiers often merely compare the holder binding with the person in front of them, perhaps glance at easy-to-check context details (e.g., expiry date), and rely on a general feeling of how such a document should look like. To warrant any real assurance in the claims, however, each of these factors needs to be thoroughly verified. In particular, all aspects of the issuer binding need to be checked, which requires up to date knowledge about the details involved; and the same goes for the authentication context. Especially the latter will, at least in case of licenses, typically include an interaction between verifier and issuer, e.g., to make sure a credential with that specific registration number actually exists.

  [^fn-issuer]: Note that this concern also holds in the other direction: information about the issuer, included in a credential, might reveal information about the subject or holder to the verifier. This is in particular the case when an issuer provides only one type of credential, and/or is the only issuer providing such credentials. Privacy in this direction is much harder to achieve. The implicitly revealed knowledge might partially be reduced by grouping multiple of issuer together -- possibly even using shared key materials [@openid:vci]. However, since the issuer is the root of trust for the verifier, this mitigation might lead to dilution of trust in the ecosystem [@openid:security]

Again, these are strong statements. The principal can indeed present credentials directly to the relying party; but this is technically not different from the OIDC authorization code flow: the ID token passes from the issuer, through the user agent -- here a browser instead of a wallet -- to verifier. Furthermore, in both architectures, both the principal and the verifier have to interact with the issuer.

First, the principal must -- at least at some point -- **retrieve the credential** from the issuer, to store it in their wallet. In theory, this could happen long before the participant presents the information to the relying party -- contrary to OIDC, where short-lived ID tokens are requested just-in-time. However, the specifications stress that for *privacy considerations*, wallets "should not store credentials longer than needed" [@openid:vci]. In fact, since "presentation sessions \[...] can be linked on the basis of unique values encoded in the credential," wallets are advised to use "a **unique** credential per presentation or per verifier" [@openid:vci] to avoid such correlation -- "each with unique issuer signatures \[and] keys" -- and then discarding the credential [@openid:vp].[^fn-considerations]

  [^fn-considerations]: These considerations also negate OpenID's claims that their specifications make technical implementation easier, simpler, and more scalable, or that they would lead to a more seamless user experience [@openid:trustmodel]. Likewise, they make it hard to get any benefit from on-device storage with hardware-backed encryption, e.g., as advocated for in their draft specification on security and trust in OpenID4VCI ecosystems [@openid:security].

Second, in order to **verify the issuer's signature**, the verifier will have to contact the issuer *upon receiving each presentation*. While it is possible to cache some of the issuer's verification material (e.g., public keys), this is equally true of (other) federated systems like OIDC. Moreover, caching becomes useless in case the issuer and wallet follow the advise to use a *unique key* per credentials and a *unique credential* per presentation (cf. supra).

Finally, even if OpenID's architecture would successfully prevent issuers from learning about the user's activity at relying parties, it would have achieved this by introducing a new intermediary entity -- the wallet -- who will possess the same information instead. Whether this is desirable or not, will depend on the context. OpenID's design, while possibly somewhat reducing the frequency of direct, synchronous interactions, it is fair to say that their strong privacy claims are at least an exaggeration. To the contrary, the considerations regarding the risk of correlation -- present in the specifications themselves -- exemplify that the proposed flows might in fact not be desirable for privacy after all.

---

In this section we critically addressed each of the benefits, proclaimed by OpenID to constitute a user-centric paradigm shift in digital identity, to conclude that most if not all of their claims are either plainly incorrect, or exaggerated, or already present in other decentralized (federated) solutions. Moreover, we pointed out that some of the so-called 'strong points' in fact hardly make up for certain less desirable effects of the new architecture. Few, if any, of OpenID's strong claims therefore survive adequate scrutiny, calling into question whether their specifications are truly a good choice for implementing the EU's regulations.

As such, there is nothing wrong with a specification that is narrowly tailored to handful of specific use cases. Even with all its limitations and vulnerabilities taken into account, OpenID's architecture still largely supports classic scenarios of credential exchange -- also offered by more traditional decentralized identity models. However, as a foothold for the EUDI infrastructure, OpenID's design drastically limits the potential of the EU's strategy, both in its initial capabilities as in its (backwards compatible) options for evolution.


# The politics of control {#sec-politics}

Having established OpenID's inability to live up to its paradigm-shifting promises, we take a look at a number of choices made by the EU's regulations themselves. From their promotion materials, to the regulations' recitals, it is clear that the EU's vision is also full of big promises: "Union citizens and residents in the Union should have the right to a digital identity that is under their *sole control*" [@eu:eudi,art.3]. Wallets aim to give them "*full control** on what data they share to identify themselves with online services \[...] at all times" [@eu:qa]. Users will also be able to **share digital documents**,[^fn-documents] and "**prove statements** \[i.e., specific personal attributes] about themselves and their relationships **with anonymity** (i.e. *without revealing identifying data*)" [@eu:qa].

  [^fn-documents]: It remains to be seen whether the terminology used in the regulations is actually compatible with the exchange of documents that are not credentials of attributes about the wallet user -- who is both holder and subject. We reiterate that the choice for the OpenID architecture, as implementation of the EUDI framework, in any case fails to deliver this promise, since the design of OpenID4VCI makes it technically impossible (cf. [@sec-vci]).

In other words, the EUDI regulation promises **self-sovereign identity** (SSI): a digital identity model in which each individual controls who they are on the Web, i.e., what information they are associated with when interacting with online services.[^fn-sovereign] Note that the SSI model does not imply **ownership** of the identity information. The data itself can originate with another party, and made available to the individual; the latter thus does not control the availability of the information. This is echoed in the OpenID trust model, which emphasizes that "it is still up to the verifier to decide whether to accept those credentials \[and] it is still up to the issuer to decide whether to issue the credential to the \[individual] in the first place \[or] to revoke and invalidate the credential" [@openid:trustmodel].[^fn-nonsovereign]

  [^fn-sovereign]: Contrasted with "non-sovereign" accounts, controlled by specific services or large, centralized identity providers, this alternative is made possible by, amongst others, public key infrastructure, federated architectures, peer-to-peer connections, and distributed ledgers. Within the EU, research into these technologies was funded in particular through the European Self-Sovereign Identity Framework (eSSIF) Lab: [@essif:project].

  [^fn-nonsovereign]: Surprisingly, OpenID takes this to imply that "verifiable credentials are **not equivalent to self-sovereignty**, when it implies the \[individual's] autonomy and freedom from issuers and verifiers" [@openid:trustmodel].

The sovereignty of the individual therefore lies in their power to autonomously control who can access which of the *available* information under which circumstances. Even with an ideal technological implementation, however, it remains to be seen whether the legal requirements, formulated in the EUDI regulation, are actually compatible with such a form of self-sovereignty. In the next sections, we will look into the concept of trusted lists, and why they are both a necessity and a risk for internet freedom and security.


## Trusted lists {#sec-lists}

Proposed by the European Telecommunications Standards Institute (ETSI) [@etsi:trustlists], **Trusted lists** are perhaps the most pervasive regulatory device introduced in eIDAS, and -- in particular -- EUDI. These lists, published and maintained by *Trusted List Providers* (TLPs), contain the trust anchors (identifiers and public keys) of regulated *Trust Service Providers* (TSPs). The *Trust Services*, which these entities provide, are *digital identity solutions* [@oecd:govdi]: (electronic) services involved in authentication processes, such as the issuance, validation, and verification of signed attestations (certificates); as well as their life cycle management, and that of their verification material (e.g., timestamps, ledgers, signatures, and seals) [@eu:eidas;@eu:arf].[^fn-trust]

  [^fn-trust]: Without diving to deep into the nature of **trust** -- which is also undefined in the regulations -- and given the techno-economic context, we take it to be a relationship between two parties, that supports the exchange of data and services [@idlab:trust]. In [@sec-politics], we look into this in more detail.

In particular, the European Commission maintains trusted lists of different types of TSPs -- including wallets, attestation providers, signature services, and other certification authorities -- that are granted the **qualified** status,[^fn-accesscert] and are thereby approved to provide **qualified trust services** [@eu:arf].[^fn-attestations] To be put on a these lists, TSPs need to adhere to certain requirements [@eu:eudi,Annex V], and be registered by a *registrar*, i.e., the supervisory body of their Member State, which in turn notifies the European Commission [@eu:notifications;@eu:certifications]. In the case of attestation providers, this procedure has to happen anew on every change in the issued credentials, since the registered data includes "the attestation type(s) that the provider intends to issue to wallet units" [@eu:arf].

  [^fn-accesscert]: While there is no trusted list for relying parties -- because their expected number would make this "practically infeasible" -- they are subjected to a similar system, in which they "\[receive] an access certificate from an *access certificate authority*, \[which] allows a wallet unit to authenticate \[them]" [@eu:arf].

  [^fn-attestations]: To be precise, the eIDAS and EUDI regulations distinguish different types of attestations, each of which is assigned a **level of assurance** (LOA), indicating the extent to which the relying party can be confident in the claimed information, determined by the digital identity solutions employed by its provider [@eu:assurance;@eu:eudi;@oecd:govdi]. Non-qualified EAAs can only be issued with LOA 'low' to 'substantial', while qualified EAAs (**QEAAs**) are the only ones that can have LOA 'high'. The legal significance of this difference is big: in contrast to EAAs, QEAAs are *legally equivalent* to documents with a handwritten signature. Two categories of QEAAs are treated separately: *official documents*, issued by a *public sector body* responsible for an *authentic source* (**PuB-EAAs**); and *Person Identification Data* (**PID**), i.e., official *identity documents* that "enables the establishment of the identity of a natural or legal person" [@eu:eudi,@eu:attestations].

Surprisingly, this pervasive aspect of the regulation did not meet much general opposition. One specific use of trusted lists, however, caused an intense debate between the legislature and the internet community -- in particular with browser vendors and advocates for net-neutrality. The topic under discussion was the **qualified website authentication certificate** (QWAC): an electronic attestation, in the form of a TLS certificate[^fn-tls] -- or an attribute certificate cryptographically linked to one (ac-QWAC) [@ietf:rfc5755] -- and issued to EU-based, audited TSPs that comply with a number of "minimal security and liability obligations" [@eu:eudi].

  [^fn-tls]: A Transport Layer Security (TLS) certificate encrypts one entire internet connection, on a lower architectural layer than the application layer, thereby protecting the integrity of the data transmitted between the client and server applications using that connection. Since TLS certificates are linked to domain names, they also prove the origin of the data, and are therefore used to authenticate the domain owners to the user.

Supported by a report from the European Union Agency for Cybersecurity (ENISA^[Previously known as the European Network and Information Security Agency.]), on critical improvements for website authentication [@enisa:qwacs], QWACs were introduced in eIDAS as a voluntary choice for websites. Their intent to "provide a means by which a visitor to a website can be assured that there is a genuine and legitimate entity standing behind the website," and as such "contribute to the building of trust and confidence in conducting business online" [@eu:eidas]. The regulation only became an issue after a crucial amendment in EUDI [@eu:eudi, art. 45], requiring browsers to treat QWAC providers as root CAs -- and thus recognize and display these government-approved certificates to their users as an assurance of trustworthy services -- *regardless of the well-established security measures traditionally upheld by the browsers* [@cepis:eudi]. In the following sections, we explore the two main consequences of this decision, that caused the worldwide criticism against it.[^fn-exception]

  [^fn-exception]: In response to the criticism, the following paragraph was added to the final EUDI regulation: "In order to contribute to the online security of end-users, providers of web-browsers should, in exceptional circumstances, be able to take precautionary measures that are both necessary and proportionate in reaction to substantiated concerns regarding security breaches or the loss of integrity of an identified certificate or set of certificates" [@eu:eudi]. This amendment should ensure that the regulation "\[does not] affect the freedom of web-browser providers to ensure web security \[...] in the manner and with the technology they consider most appropriate" [@eu:eudi]. However, the paragraph only appears in the preliminary recitals of EUDI, and is thus -- unlike the article itself -- not legally binding [@cepis:eudi].


## A weakened internet security {#sec-weakened}

TLS certificates can validate several characteristics of an internet connection, including the domain name and organization linked to the website. Like with any certificate, trust in TLS verification is dependent on trust in the CA vouching for it. Since users can hardly be expected to know which of the many CA's to trust on the Web, browser vendors take up this burden by maintaining a 'trusted list' of vetted CAs, typically based on advice of the Certification Authorities and Browsers (CA/B) Forum [@isoc:impact].[^fn-rootstores] In particular, until the early 2020s, websites complying with a strict set of Extended Validation (EV) rules -- compiled by this CA/B Forum -- were displayed with a 'green shield' indicator in the address bar of many browsers, to indicate such delegated trust to the user.

  [^fn-rootstores]: These lists of trusted CAs, also called *root stores*, are carefully compiled based on well-defined, transparent **root programs**: security standards, including policies and safeguards concerning encryption, public transparency, audits, and other practice requirements [@mozilla:position].

The eIDAS certificates, and EUDI's obligation to display them in browsers, copy this design, but relies on government certification instead of a multi-stakeholder CA architecture. However, while EUDI literally states that "the results of existing industry-led initiatives, for example the \[CA/B] Forum, have been taken into account" [@eu:eidas], the CA/B community -- including most major browsers -- had in fact already deprecated or discontinued the EV system by the time EUDI was drafted. They moved away from this practice after research by Google and UC Berkeley had shown that the indicators did not have a significant effect on the behavior of users: they "\[did not] provide users with clear, actionable cues about online trust, and were therefore adding cost and complexity but little or no benefit" [@isoc:impact].

The cost--benefit analysis was, however, not the reason for the strong reaction the regulation triggered. Responding to the proposal's feedback rounds [@eu:feedback1;@eu:feedback2], the Common Certification Authority Database (CCADB) [@ccadb:qwacs] -- an initiative of the Linux Foundation -- together with major browsers, argues that mandating QWACs undermines technical neutrality, interoperability, and user privacy -- principles that are central to the intent of eIDAS itself. Similar objections were later raised in an impact brief from the Internet Society (ISOC), who claim that "ETSIโs assumption that browsers would add eIDAS-approved \[TSPs] to the trusted root list violates \[the ideal of an open, trustworthy Internet]," based on principles of collaboration, expertise, transparency, and consensus around "trust criteria mutually agreed among \[...] relevant stakeholders" [@isoc:impact].

In a position paper [@mozilla:position], reiterating the objections of their earlier response [@mozilla:response], Mozilla calls the new regulation an 'unprecedented move', which "will amount to forced website certificate whitelisting by government dictate and will irremediably harm usersโ safety and security" [@mozilla:response;@mozilla:position]. They emphasize that the proposal "goes against established best practices \[...] created by consensus from the varied experiences of the Internetโs explosive growth \[which] have successfully helped keep the Web secure for the past two decades" [@mozilla:response]. The obligation for browser vendors to automatically include TSPs in their root programs, would effectively "replace the security expertise of major browser companies \[...] with legislation premised on weaker and discredited security architectures," leading to "a regression in the security assurances that users have come to expect from their browsers" [@mozilla:position]. The requirements for inclusion in, for example, Mozilla's root program, are more rigorous, and more transparent, providing for more public oversight, and more stringent audits than eIDAS' criteria for TSPs [@mozilla:response,@mozilla:position]. Therefore, "by mandating that TSPs be supported by browsers in general, and in particular when they fail to meet the security and audit criteria of their root program, \[EUDI] will negatively transform the website security ecosystem in a fundamental way" [@mozilla:position].

According to ISOCs impact brief, these negative effects come in two forms: by issuing incorrect certificates, and through inability to rapidly address security incidents [@isoc:impact]. This risk assessment is echoed in an open letter published by the Electronic Frontier Foundation (EFF), signed by numerous cybersecurity researchers, advocates, and practitioners [@eff:letter]: "allowing some website certificates to bypass existing security standards \[...] increases the risk that insecure or malicious certificates will be issued \[...] and make (sic) it impossible for the cybersecurity community to quickly respond when certificates are found to pose a risk" [@eff:letter]. In general, the EFF characterizes the regulation as "a dangerous cybersecurity policy trend," which goes against established norms in cybersecurity and risk management, and "compels private actors to forgo their duty to those who use their products and services, by assuming that because government-appointed Certificate Authorities are subject to government security standards, they can pose no cybersecurity risk" [@eff:letter].


## (Re)centralized (dis)trust {#sec-recentral}

The trusted lists concerning website authentication share a lot of similarities with their counterparts for providers of wallets, attestations, and signatures -- and, in fact, with any kind of eIDAS/EUDI lists.[^fn-keys] In general, each of these trusted lists, as mandated by these regulations, institutionalizes an *elevated status*, granted to a select group of TSPs by *national government registrars* [@eu:certifications]. This nature of the EUDI framework entails both economical and political risks.

  [^fn-keys]: For example, the promotion of a central key management system (CKM), through lists of trusted signature services, would have the majority of citizens entrust cryptographic information (e.g., private keys) -- underlying their participation in the EUDI ecosystem -- to government-selected organizations. Without careful, open, industry-led security measures, this will undermine citizen's autonomy and control, rather than bolster it. Consequences of such approaches to "let there be trust" by government decree can, for example, be found in the aftermath of similar legislation in Uruguay [@sabiguero:trust].

The EUDI design allows only registered systems -- abiding by the criteria set forth in the regulations -- to participate in the EUDI ecosystem, and interact with other parties via the architecture's protocols [@eu:protocols]. Therefore, "those who are on the list receive economic advantages; those who are not on it have disadvantages" [@schall:eudi]. In itself, this is not an issue. However, several of these requirements could form *economical obstacles* (e.g., increased liability, financial minima) or *technical hurdles* (e.g., renewal on each change attestation) [@eu:certifications;@eu:protocols]. This could lead to an ecosystem that *favors affluent organizations*, strengthening their *privileged position* in their respective techno-economic spheres, and leading to vendor lock-ins and gate-keeper behavior [@schall:eudi]. Rather than a decentralization of digital identity, such an ecosystem could therefore potentially result to a (re)centralization, in which authentication on the Web is directed by a small number of key economical and political actors.

Importantly, this not only holds for the TSPs themselves, but *even for relying parties* making use of their services [@eu:relyingparties]. Even to obtain the simplest attestation from a wallet, a relying party would need to jump through the hoops described above, *regardless of the choice of the wallet user*. This conclusion definitely *falls short of the 'user-oriented identity management'* -- to give users control over their own digital identity and personal data -- promised by the regulations. Indeed, while "people can choose which bits of their identity information to share" [@igrant:eudi], through consent and selective disclosure, they have *nothing to say* about which issuer can provide their credentials, nor to which verifiers they can present their attributes. A wallet then merely serves as a uniform interface, through which (only) registered parties -- willing to pay the cost of entry -- can exchange information in a regulation-compliant manner. As such, a wallet user can hardly be said to be 'in charge' of their personal data.

Moreover, many of the implementing regulations require TSPs to *log and preserve information* about their systems and the parties they interact with [@eu:information;@eu:breaches;@eu:matching]. In certain cases, e.g., security breaches, TSPs have the obligation to *notify the European Commission* -- upon which the latter can decide to *suspend the service/provider* from the ecosystem. These requirements are not limited to information about TSPs, wallets, and relying parties, however: in some cases -- e.g., *identity matching* by cross-border services -- *personal information* about the wallet user, or subject of authentication, must also be logged.

Given EUDI's possible tendency towards a structure with privileged positions, these increased logging requirements could pose an additional risk of abuse, such as *monitoring the usage of wallets* to *profile citizens' behavior* and interests; either by malicious TSPs themselves, or as a target of external actors. Such risks not only negate OpenID's emphasis on minimizing unnecessary data disclosure [@openid:vci;@openid:vp;@igrant:eudi]; they also stand in *sharp contrast with other EU legislation*, including data protection regulations like the GDPR, and the EUDI regulation's own insistence on **unlinkability**. The latter is particularly surprising, since it is part of the same regulation, and specifically aims to prevent *any* party to bring together "data that allows for tracking, linking, correlating or otherwise obtain knowledge of transactions or user behavior *unless explicitly authorized by the user*" [@eu:eudi].[^fn-unobservability] This is precisely the tension pointed out by the expert group for Legal and Security Issues (LSI) of the Council of European Professional Informatics Societies (CEPIS): "Unfortunately the development of the Architecture Reference Framework (ARF) \[...] is not transparent, but behind closed doors, and available drafts do not support the legal requirements for safeguards for unlinkability" [@cepis:eudi].

  [^fn-unobservability]: Similar to the case of QWACs, the regulators' response was limited to a minor addition to the (not legally binding) recitals of the final EUDI regulation [@eu:eudi]. While this once more emphasizes the need for *unobservability* of a user's transactions by wallet providers, it in no way answers the question of its technical implementation, given the practically opposite requirements [@cepis:eudi].

Most importantly, however, the economical and political design of EUDI, as described above, allows governments to whitelist favored service providers, without much restriction. CEPIS criticizes that there are "no safeguards that prevent the governments \[...] from exercising surveillance over everything its users do with it" [@cepis:eudi]. It is important to realize that these scenarios are not 'horror stories', but rather "deductions from already known types of attack, economic incentives and \[...] historical experience" [@schall:eudi]. Organizations managing CA root programs, like Mozilla, have first hand experience with these dangers. They list, amongst others, the governments of Mauritius, Kazakhstan, and China, as "authoritarian regimes \[who] have long sought to override \[trusted list] policies" [@mozilla:position].

Note that none of the critics referred to above are against the concept of trusted lists. In fact, they are an **architectural necessity** in any decentralized system. At the same time, however, they are an instrument of power, which should urge us to ask questions like: "In whose hands does this tool fall?", "How strong are the barriers against misappropriation, commercialization, and criminal exploitation?", "How is abuse prevented if political or economic interests are involved?" [@schall:eudi]. After all, "a globally connected Internet is premised on the ability of Internet users to access and use resources in other networks *without unnecessary restrictions*" [@isoc:impact]. Giving political actors the power to directly influence such restrictions -- either by imposing them or by overruling them -- therefore sets a dangerous antecedent.


# Alternative solutions {#sec-alt}

In this final section, we highlight some of the alternative solutions already mentioned earlier, and propose a number of additional lines of research worth looking in to. Since immediate legislative changes are improbable, we focus on technological specifications that might overcome some of the issues we raised, and pave the way towards a truly self-sovereign digital authentication framework.

To achieve this, we argue that every solution should be aimed at a broad interpretation of authentication and identity, as put forward in [@sec-char] -- or at least provide sufficient points of extensibility that enable probable evolution paths towards it. The core principles of the semantic Web can therefore function as a good starting point: global identifiers -- URIs (and in particular DIDs) [@ietf:rfc3986,@w3c:did] -- combined with semantically rich structures like RDF [@w3c:rdf], allow for a far-reaching interoperability with existing and future technologies.

Based on this strong foundation, **W3C's models** of *verifiable credentials and presentations* [@w3c:vc] -- also supported by OpenID -- offer the most potential for aligning the wide variety of electronic attestations of attributes. As we pointed out in [@sec-trust], they are not inherently more expressive or secure than classic OIDC tokens, but they achieve those features in a semantically richer, more interoperable, and extensible manner. On the other hand, while W3C VCs *can handle partial identity* -- e.g., role-based, pseudo [@idlab:pseudo] -- they are still targeted to a subject, and can therefore *cannot express certificates* in the broader sense. A compatible model for such general 'attestation documents' therefore remains a crucial topic for future research.

Throughout [@sec-arch], we already mentioned several other OpenID specifications that together are functionally equivalent to OpenID4VCI and OpenID4VP. Based on the well-established OIDC [@openid:connect], SIOPv2 aligns credentials with self-issued presentations [@openid:siop], OIDC4IDA adds a levels of assurance model [@openid:ida;@openid:idaschema;@openid:idareg], and OIDC Claims Aggregation can substitute for wallet-like behavior [@openid:aggregation]. Moreover, these specifications are better aligned, and do not redundantly duplicate equivalent models from each other or from external sources.

However, we also pointed out several problems with the OpenID ecosystem in general, which should be addressed in any possible alternative. In particular, attention needs to be payed to best practices in internet security and software design -- such as the separation of orthogonal concerns. Ideally, an alternative solution would therefore be based directly on OAuth 2.1 [@ietf:oauth21]. This would lift OpenID's restriction to a single endpoint, and instead allow the immense variety of attestations to be exchanged via any kind of interface: through classic RESTful APIs, and 'social data' (e.g., Solid), but also through direct database queries, or web streams.

To access such heterogeneous forms of attestations, however, another issue needs to be addressed first. In [@sec-vci] and [@sec-vp], we criticized the inadequacy of a single string to express the entire semantics of each type of credential -- even for the limited variety exchangeable through OpenID wallets. We also pointed out the issues with OpenID's several custom, non-interoperable query languages, e.g., DCQL and claim descriptions. Within the restrictions of OpenID, the JSONPath or JSON Schema specifications [@ietf:rfc9535;@ietf:jsonschema] offer a more standardized, interoperable alternative. However, as we have pointed out in [@sec-oidc], even a JSON-based approach has a limited flexibility and interoperability in the light of a more heterogeneous range of credentials. The suggested addition of SPARQL queries [@w3c:sparql], to request specific claims from the combined credentials in a wallet, would already make a big difference [@idlab:queryvc]. In order to also include other data interfaces, research into a more abstract query (meta-)language is necessary.

A choice for OAuth would also open up the possibility of using its *User-Managed Access* (UMA) extension [@uma:grant;@uma:fed]. By modeling a dynamic negotiation with the verifier, UMA enables more complex authorization contexts to be established, thereby lifting OpenID4VCI's limitation to static, preset credential configurations (cf. [@sec-vci]). UMA also emphasizes asynchronous interaction with the principal, which opens up the way for use cases involving automation. In earlier work, we already discussed UMA in more detail as an alternative to approaches involving OIDC and access control lists [@idlab:uma]. We also provided a profile specification for UMA, called *Authorization for Data Spaces* (A4DS) [@idlab:a4ds], which puts forward an authorization model in which wallets let users regulate access to data that remains safely at its original source -- functioning more like a remote key than like a hard drive.

Based on the pointers in this section, one interesting project to follow is the *Grant Negotiation and Authorization Protocol* (GNAP) [@ietf:rfc9635]. This draft specification -- sometimes hailed as 'OAuth 3.0' -- combines the two decades of best practices around Oauth 2.0 with an OIDC-like identity provider and an authorization model inspired by UMA. It is important to keep in mind, though, that neither GNAP, nor any other alternative, will be able to actually fulfill the promise of a decentralized, self-sovereign European identity framework, as long as the risks in [@sec-politics] -- caused by the economic and political impact of trusted lists (cf. [@sec-lists]) -- are not addressed. Only through transparent collaboration with experts and stakeholders can the technological potential of these solutions be realized.


# Summary and conclusion {-}

We started out from the observation that, in legal and technical sources pertaining identity in data spaces -- in particular those related to recent EU regulations -- the term '**identity**' is ill-defined. Aggregating several older international standards' glossaries, in [@sec-char] we settled on a workable definition: *an identity is a subset of an entity's characteristics, sufficient to represent and recognize that entity*. We also clarify the concept '**authentication**', which is the *process of determining the authenticity of information, by verifying its origin and integrity*, often through a (cryptographic) binding between multiple claims. In [@sec-trust], we explained the roles of these concepts in the trust relations of the *issuer-holder-verifier* model. We also showed how practically all verifiable information -- any certificate -- can be identity information (i.e., a credential); in particular in the light of *context-dependent* (partial) identities like roles, pseudonyms, and anonymity.

Applying our findings to *OpenID's architecture* for the EUDI framework, in [@sec-arch] we exposed several issues with OIDC, OpenID4VCI, and OpenID4VP, indicating a mismatch between a general conceptualization of identity and the narrow capabilities of those specifications. Implementing the EUDI framework in an **OIDC-based** architecture precludes a healthy separation of concerns, leaves the door open for insecure practices, and immediately limits the flexibility of the provided interfaces (cf. [@sec-oidc]). While **OpenID4VCI** and **OpenID4VP** replace identity tokens with credentials, their reliance on preset configurations, lack of expressivity in credential types, limited query language, and format-dependent claim semantics drastically narrows the use cases in which the specification can be applied. Furthermore, since their credentials must have an (interactively) identified subject, the possibilities for automated scenarios are limited (cf. [@sec-vci] and [@sec-vci]).

In [@sec-paradigm], we discussed the strong *parallel* between OpenID4VP and OpenID4VCI, comparing it to self-issued credentials in SIOPv2 (OIDC), and self-asserted claims in W3C's Verifiable Presentations. We find that the latter specifications build direct, holder-based issuance *on top of* -- and compatible with -- their respective third-party issuance flows, while the former specifications provides similar functionally through *almost identical yet incompatible mechanisms*. Looking for the rationale behind this double tiered design, we discussed the **paradigm shift** -- claimed by OpenID -- away from federated models and towards user-centricity. We refuted its claimed increase in user *control, privacy, and portability* of identity information; and concluded that the OpenID trust model in no aspect goes beyond existing decentralized approaches:

  - While an increase in *offline functionality* could be attributed to OpenID's new architecture, the BYOB **portability** it proclaims is not a real innovation, since this benefit is already present in many other decentralized systems -- including OpenID's own OIDC. \[[@sec-portability]]

  - The same goes for the facilitation of *informed consent* to bolster people's **control** over their personal information. *Selective disclosure*, on the other hand -- providing control over individual claims in larger credentials -- *predominantly applies to scenarios that arise from OpenID's design* itself. In other decentralized models, this form of data minimization is much less of a necessity. \[[@sec-control]]

  - The introduction of a user-bound wallet also doesn't provide the promised increase in **privacy**. OpenID provides no reason why a wallet provider would protect the private information in credentials better than their issuer. Moreover, this new intermediary does not truly make *direct interaction* between issuers and verifiers redundant -- contrary to OpenID's claims. In fact, when taking into concern OpenID's own privacy considerations, the resulting flow is *practically identical to OIDC*. \[[@sec-privacy]]

From this analysis, we concluded that OpenID's specifications offer a technological framework that is tailored to a specific set use cases -- including most *classic scenarios of credential exchange* -- without necessarily offering more than other decentralized identity models. Since this *limits the capabilities and interoperability* of the ecosystem, we called into question whether OpenID is truly a good choice as foothold for EUDI.

In [@sec-politics], we therefore looked into the European regulations themselves, and critically assessed a number of legislative choices, and their impact on the EU's promise of *self-sovereign identity*. In [@sec-lists], we explained the introduction of trusted lists, assurance levels, and qualified service providers; and their increasingly strong constraints -- and ties to national governments -- through the EUDI's amendments to the eIDAS regulation. As a concrete example, in [@sec-weakened] we highlighted the commotion in the internet community around *qualified website authentication certificates* -- called out by many as a dangerous trend in cybersecurity policy, that weakens global internet security, by violating established norms on net-neutrality and privacy. Opening up these implications to trusted lists in general, we emphasized the risks of making *participation in an advantageous market* dependent on a registration procedure that forms an *economical cost of entry*, and is *governed by political institutions*. The economic and political incentives in such ecosystems not only risk a de facto **re-centralization of digital identity**, driven by *gate-keeper behavior* of privileged market players; but, without sufficient safeguards, they are also vulnerable to government abuse.

On an individual level the regulations fall equally short of their goal. As we also discussed in [@sec-recentral], because of the exclusory ecosystem, *people are severely restricted* in what to do with their personal information. Being restricted to registered parties, they *cannot freely choose* which issuers and verifiers to interact with, even if the desired parties technically support the necessary architecture. This result is hardly the promised *'user-oriented identity management'* in which individuals are in any way *'in charge'*. Moreover, given the regulations' increased requirements for *logging and preserve information* -- and disclosing some of it to government institutions -- this tendency towards re-centralization poses the additional risk of *monitoring citizens' behavior and interests*, regardless of the privacy-preserving capabilities of the chosen technical framework. This stands in sharp contrast with other legal requirements (e.g., GDPR), and promises about the *unlinkability* of correlating data.

We wrapped up this paper in [@sec-alt] by arguing that the risks related to EUDI will not go away before the regulators revise its problematic construction of trusted lists. In anticipation of that, we gave a number of suggestions for research into technical solutions -- aimed at a broader interpretation of authentication and identity -- that overcome some of the issues with the architecture of OpenID. Leveraging the extensibility and interoperability of semantic Web models (URIs, DIDs, RDF), we take W3C VC's to be a good foundation, even though they lack support for attestation documents that are not subject-based. We repeated the functional equivalence of OpenID4VCI and OpenID4VP with a combination of OIDC, SIOPv2, OIDC4IDA, and OIDC Claims Aggregation. However, we advocate for an architecturally sounder framework based directly on OAuth, which has much broader applicability, especially when extended with the asynchronous and dynamic features of UMA, or in its evolution path through GNAP. Future research should build on these, in line with the A4DS profile, and needs to look into query (meta-)languages that are better suited to uniformly request attestations from a more heterogeneous range of providers.

---

We conclude that -- while offering a plausible architecture for the use cases which the EUDI regulations seem to allow -- OpenID's architecture doesn't offer much innovation. Both the use cases and the highlighted technical features are achievable with other existing decentralized technologies. Without a thorough analysis of (digital) identity, the regulations and their architecture have restricted themselves to a narrow intuition of what credential exchange could look like -- thereby missing the opportunity to construct a truly uniform model of authentication on the Web. Moreover, with the choice for OpenID's technical framework, the EUDI ecosystem inherits an architectural history of ignoring best practices in software design, internet security, and interoperability -- without any significant innovation to speak for it, neither on a technical level (e.g., semantic expressivity, query languages, automation) nor with respect to the promised increase in control, privacy, and portability of identity information.

Equivalent solutions already exist today in OIDC extensions on self-issuance, identity assurance, and claims aggregation. Alternatives that aim at a broader interpretation of authentication and identity, can be found in OAuth 2.1 with UMA, or GNAP. In either case, future research is needed to achieve less trivial use cases; in particular into uniform query (meta-)languages.

The question we are left with, however, is whether a more capable technical framework would be beneficial at all, given the restrictions imposed by the EUDI regulations themselves. The manner in which the regulators have institutionalized EUDI -- and in particular the trusted lists -- risks to create perverse economic and political incentives. Rather than building an inclusive decentralized ecosystem of self-sovereign authentication on the Web, this could lead to a re-centralization of digital identity, and to dangerous vulnerabilities for abuse, such as profiling of citizens' behavior. In its current state, the EUDI ecosystem therefore has everything to become restriction of people's choice and control, rather than an environment in which they feel in charge over their personal information. Authentication then becomes "an entry ticket to a system in which every transaction is clearly assigned to a person" [@schall:eudi].
